{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Proposal\n",
    "Jordan Chong\n",
    "May 8th, 2019\n",
    "\n",
    "## Proposal\n",
    "\n",
    "\n",
    "### Domain Background\n",
    "\n",
    "Much of the attention that is giving to deep learning's achievements has been directed towards it's ability to make sense of a complicated world using visual data such as images and video. However cameras are expensive and have a limited field of view. Audio data on the other hand is cheaper to collect and comes with more freedom in terms of positioning. It is said that a team of world class machine learning engineers with mediocre data sets can often be outperformed by mediocre machine learning engineers with stellar datasets. For this reason it appears to be important that we make certain we are making effective use of one of our cheapest and readily collectible sources of data, audio.\n",
    "\n",
    "Part of the reason is due to the fact that audio data, particularly of the non-speech variety, is slightly more difficult to label and more susceptible to noise. And due to the wide range of sounds we can expereince and the variety of sources that create similar sounding noises we do not currently have a reliable automatic general-purpose audio tagging system. With the datasets provided by Freesound and Google's Audioset, much of the manual labor of labeling audio clips has been done and it is predicted that we may have success in developing multi-label classifiers built in the same vein as those for imagenet. \n",
    "\n",
    "### Problem Statement\n",
    "_(approx. 1 paragraph)_\n",
    "\n",
    "The goal of this project will be to develop an algorithm to tag audio data automatically using a diverse vocabulary of 80 categories. The hope will be to design the algorithm in such a way that it might become usefull for applications such as automatic labelling of sound collections to the development of systems that automatically tag video content or recognize sound events happening in real time. \n",
    "\n",
    "### Datasets and Inputs\n",
    "\n",
    "The Music Technology Group from the University of Pompeu Fra in Barcelona ([MTG-UPF](https://www.upf.edu/web/mtg)) can be thanked for having compiled the Freesound Dataset [FSD](https://annotator.freesound.org/fsd/) which was based on Freesound content organized with the Google's AudioSet Ontology. It contains everyday sounds such as musical instruments, human sounds, domestic sounds, and animals. This data was labeled manually by humans following a data labeling process using the Freesound Annotator platform. \n",
    "\n",
    "This data will be supplemented by additional \"noisy\" tracks taken from a pool of Flickr videos taken form the [Yahoo Flickr Creative Commons 100M dataset (YFCC)](http://code.flickr.net/2014/10/15/the-ins-and-outs-of-the-yahoo-flickr-100-million-creative-commons-dataset/). This data was labeled using automated heuristics applied to the audio content and metadata of the original Flickr video clips. For this reason the YFCC data should be expected to be much noisier.\n",
    "\n",
    "The audio data is labeled using a vocabulary of 80 labels from [Googleâ€™s AudioSet Ontology](https://research.google.com/audioset////////ontology/index.html). It includes various sounds such as Guitar and other Musical instruments, Percussion, Water, Digestive, Respiratory sounds, Human voice, Human locomotion, Hands, Human group actions, Insect, Domestic animals, Glass, Liquid, Motor vehicle (road), Mechanisms, Doors, and a variety of Domestic sounds. \n",
    "\n",
    "The training data is split into 2 subsets. FSD's curated subset and YFCC's noisy subset. There are about 10.5 hours worth of audio from the curated set and 80+ hours worth of data from the noisy set. For both sets the average number of labels per clip is 1.2. Clips can range from 0.3 seconds to 30 seconds in .wav format.\n",
    "\n",
    "The testing data comes entirely from the curated FSD dataset.\n",
    "\n",
    "### Solution Statement\n",
    "\n",
    "Previous Freesound Audio Tagging competitions have shown good results when classifying audio with roughly 40 labels using neural net approaches such as variations of ResNext and temporal 1D convolutions to handle raw wave audio files. By mapping raw wave input which is a 1D array to 80 different label outputs a functional solution can be achieved. Additional feature extractino improvements such as MFCC (Mel Frequency Cepstral Coefficients) will be used to enchance the input to the neural network.\n",
    "\n",
    "### Benchmark Model\n",
    "\n",
    "Ensemble methods such as random forest and adaboost have historically done well on classification problems. For this problem I intend to start with an MFCC enchaced version of adaboost as a starting point for a non neural net based benchmark model for comparison. Additionally a neural net benchmark will also be taken from previous Freesound audio tagging competitions to compare the algorithm against the current state of the art audio tagging solutions. In particular Cochlear.ai researchers from South Korea have used a single block Densenet architecture with some additional mixup training techniques to achieve %95.3810 accuracy rating on the 40 label problem. Comparing that to the 80 label set will be insightful in shaping an improved algorithm.\n",
    "\n",
    "### Evaluation Metrics\n",
    "_(approx. 1-2 paragraphs)_\n",
    "\n",
    "In this section, propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms).\n",
    "\n",
    "Accuracy alone should prove to be a sufficient evaluation metric for this classificaiton task as this is how the kaggle competition will be judged regardless. The model will be evaluated on the 1.6k sample manually labeled test dataset. The accuracy will be determined by how many of these samples can be accurately predicted based only on the wave audio input.\n",
    "\n",
    "### Project Design\n",
    "\n",
    "The begin, the project will entail doing some initial data evaluation, possibly some wrangling, and analysis. I will examine properties of the audio to judge potential for further feature ehancements or for clues in shaping the neural net architecture. Then benchmark models will be built using historical freesound compeition winning algorithms and ensemble methods to set a baseline measure for accuracy. From there I will explore optimizations that can be made to the benchmark models or experiment with entirely new neural net architectures in order to see if accuracy can be significantly improved from the benchmark model. These architectures will likely employ some sort of temporal convolutional neural net.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
